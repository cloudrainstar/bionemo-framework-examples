{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fd2cd27-9a11-4a03-a21f-5a42ea5fcca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff96d192",
   "metadata": {},
   "source": [
    "## Load TSV File as a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c333869-2de3-4e4f-8640-a13ff414a646",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"Testset\"\n",
    "types = defaultdict(lambda: \"string\", Risk=\"int\")\n",
    "df = pd.read_csv(\"./Testset.addseq.v2.tsv\", sep=\"\\t\", dtype=types)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9a7779",
   "metadata": {},
   "source": [
    "## Set Up BioNemo Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dad9925-6167-448e-beb3-fdce4e9dff60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model configuration at: /workspace/bionemo/examples/protein/esm1nv/conf\n"
     ]
    }
   ],
   "source": [
    "# Set up BIONEMO\n",
    "try:\n",
    "    BIONEMO_HOME: Path = Path(os.environ['BIONEMO_HOME']).absolute()\n",
    "except KeyError:\n",
    "    print(\"Must have BIONEMO_HOME set in the environment! See docs for instructions.\")\n",
    "    raise\n",
    "\n",
    "config_path = BIONEMO_HOME / \"examples\" / \"protein\" / \"esm1nv\" / \"conf\"\n",
    "print(f\"Using model configuration at: {config_path}\")\n",
    "assert config_path.is_dir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce8a166",
   "metadata": {},
   "source": [
    "Load the inference configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "035c4439-30c4-4717-83ba-40d0763b7044",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bionemo.utils.hydra import load_model_config\n",
    "\n",
    "cfg = load_model_config(config_name=\"infer.yaml\", config_path=config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3c43f7",
   "metadata": {},
   "source": [
    "Load the model specified in the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de522584-a1d0-449c-a2f8-9b053d02928a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:rdkit:Enabling RDKit 2023.09.5 jupyter extensions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-11-07 00:02:45 megatron_hiddens:110] Registered hidden transform sampled_var_cond_gaussian at bionemo.model.core.hiddens_support.SampledVarGaussianHiddenTransform\n",
      "[NeMo I 2024-11-07 00:02:45 megatron_hiddens:110] Registered hidden transform interp_var_cond_gaussian at bionemo.model.core.hiddens_support.InterpVarGaussianHiddenTransform\n",
      "[NeMo I 2024-11-07 00:02:45 utils:514] pytorch DDP is not initialized. Initializing with pytorch-lightening...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-11-07 00:02:45 utils:360] Restoring model from /workspace/bionemo/models/protein/esm1nv/esm1nv.nemo\n",
      "[NeMo I 2024-11-07 00:02:45 utils:364] Loading model class: bionemo.model.protein.esm1nv.esm1nv_model.ESM1nvModel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive mode selected, using strategy='auto'\n",
      "[NeMo I 2024-11-07 00:02:45 exp_manager:396] Experiments will be logged at /workspace/bionemo/examples/protein/esm1nv/nbs/nemo_experiments/ESM1nv_Inference/2024-11-07_00-02-45\n",
      "[NeMo I 2024-11-07 00:02:45 exp_manager:842] TensorboardLogger has been set up\n",
      "[NeMo I 2024-11-07 00:02:45 utils:333] \n",
      "    \n",
      "    ************** Trainer configuration ***********\n",
      "[NeMo I 2024-11-07 00:02:45 utils:334] \n",
      "    name: ESM1nv_Inference\n",
      "    desc: Minimum configuration for initializing a ESM1nv model for inference.\n",
      "    trainer:\n",
      "      precision: 16-mixed\n",
      "      devices: 1\n",
      "      num_nodes: 1\n",
      "      accelerator: gpu\n",
      "      logger: false\n",
      "    exp_manager:\n",
      "      explicit_log_dir: null\n",
      "      exp_dir: null\n",
      "      name: ${name}\n",
      "      create_checkpoint_callback: false\n",
      "    model:\n",
      "      micro_batch_size: ${model.data.batch_size}\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      seq_length: 512\n",
      "      max_position_embeddings: 512\n",
      "      encoder_seq_length: 512\n",
      "      num_layers: 6\n",
      "      hidden_size: 768\n",
      "      ffn_hidden_size: 3072\n",
      "      num_attention_heads: 12\n",
      "      init_method_std: 0.02\n",
      "      hidden_dropout: 0.1\n",
      "      kv_channels: null\n",
      "      apply_query_key_layer_scaling: true\n",
      "      layernorm_epsilon: 1.0e-05\n",
      "      make_vocab_size_divisible_by: 128\n",
      "      pre_process: true\n",
      "      post_process: false\n",
      "      bert_binary_head: false\n",
      "      resume_from_checkpoint: null\n",
      "      masked_softmax_fusion: true\n",
      "      tokenizer:\n",
      "        library: sentencepiece\n",
      "        type: null\n",
      "        model: nemo:f9e5097b22ec4aac8849955c30fdb1c3_protein_sequence_sentencepiece.model\n",
      "        vocab_file: /tokenizers/vocab/protein_sequence_sentencepiece.vocab\n",
      "        merge_file: null\n",
      "        vocab_path: ${oc.env:BIONEMO_HOME}/tokenizers/protein/esm1nv/vocab/protein_sequence_sentencepiece.vocab\n",
      "        model_path: ${oc.env:BIONEMO_HOME}/tokenizers/protein/esm1nv/vocab/protein_sequence_sentencepiece.model\n",
      "      native_amp_init_scale: 4294967296\n",
      "      native_amp_growth_interval: 1000\n",
      "      fp32_residual_connection: false\n",
      "      fp16_lm_cross_entropy: false\n",
      "      seed: 1234\n",
      "      use_cpu_initialization: false\n",
      "      onnx_safe: false\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: 1\n",
      "      data:\n",
      "        ngc_registry_target: uniref50_2022_05\n",
      "        ngc_registry_version: v23.06\n",
      "        data_prefix: ''\n",
      "        num_workers: 4\n",
      "        dataloader_type: single\n",
      "        reset_position_ids: false\n",
      "        reset_attention_mask: false\n",
      "        eod_mask_loss: false\n",
      "        masked_lm_prob: 0.15\n",
      "        short_seq_prob: 0.1\n",
      "        skip_lines: 0\n",
      "        drop_last: false\n",
      "        pin_memory: false\n",
      "        data_impl: ''\n",
      "        data_impl_kwargs:\n",
      "          csv_mmap:\n",
      "            header_lines: 1\n",
      "            newline_int: 10\n",
      "            workers: 10\n",
      "            sort_dataset_paths: true\n",
      "            data_sep: ','\n",
      "            data_col: 3\n",
      "          csv_fields_mmap:\n",
      "            newline_int: 10\n",
      "            header_lines: 1\n",
      "            workers: null\n",
      "            sort_dataset_paths: false\n",
      "            data_sep: ','\n",
      "            data_fields:\n",
      "              id: 0\n",
      "              sequence: 1\n",
      "          fasta_fields_mmap:\n",
      "            data_fields:\n",
      "              id: 0\n",
      "              sequence: 1\n",
      "        use_upsampling: true\n",
      "        seed: 1234\n",
      "        max_seq_length: 512\n",
      "        dataset_path: ${oc.env:BIONEMO_HOME}/data/FLIP/secondary_structure/test/x000\n",
      "        dataset:\n",
      "          train: x[000..049]\n",
      "          test: x[000..049]\n",
      "          val: x[000..049]\n",
      "        micro_batch_size: 8\n",
      "        modify_percent: 0.0\n",
      "        perturb_percent: 0.1\n",
      "        batch_size: 128\n",
      "        output_fname: ''\n",
      "        index_mapping_dir: null\n",
      "        data_fields_map:\n",
      "          sequence: sequence\n",
      "          id: id\n",
      "        mask_percent: 0.8\n",
      "        identity_percent: 0.1\n",
      "        output_format: pkl\n",
      "      optim:\n",
      "        name: fused_adam\n",
      "        lr: 0.0002\n",
      "        weight_decay: 0.01\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.98\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_steps: 500\n",
      "          constant_steps: 50000\n",
      "          min_lr: 2.0e-05\n",
      "      dwnstr_task_validation:\n",
      "        enabled: true\n",
      "        dataset:\n",
      "          class: PerTokenPredictionCallback\n",
      "          task_type: classification\n",
      "          infer_target: bionemo.model.protein.esm1nv.infer.ESM1nvInference\n",
      "          max_seq_length: 512\n",
      "          emb_batch_size: 128\n",
      "          batch_size: 128\n",
      "          num_epochs: 10\n",
      "          task_name: secondary_structure\n",
      "          dataset_path: /data/FLIP/secondary_structure\n",
      "          dataset:\n",
      "            train: x000\n",
      "            test: x000\n",
      "          sequence_col: sequence\n",
      "          labels_col:\n",
      "          - 3state\n",
      "          - resolved\n",
      "          labels_size:\n",
      "          - 3\n",
      "          - 2\n",
      "          mask_col:\n",
      "          - resolved\n",
      "          - null\n",
      "          random_seed: 1234\n",
      "          optim:\n",
      "            name: adam\n",
      "            lr: 0.0001\n",
      "            betas:\n",
      "            - 0.9\n",
      "            - 0.999\n",
      "            eps: 1.0e-08\n",
      "            weight_decay: 0.01\n",
      "            sched:\n",
      "              name: WarmupAnnealing\n",
      "              min_lr: 1.0e-05\n",
      "              last_epoch: -1\n",
      "              warmup_ratio: 0.01\n",
      "              max_steps: 1000\n",
      "      global_batch_size: 128\n",
      "      precision: 16\n",
      "      target: bionemo.model.protein.esm1nv.esm1nv_model.ESM1nvModel\n",
      "      nemo_version: 1.18.1\n",
      "      downstream_task:\n",
      "        restore_from_path: ${oc.env:BIONEMO_HOME}/models/protein/esm1nv/esm1nv.nemo\n",
      "        outputs:\n",
      "        - embeddings\n",
      "        - hiddens\n",
      "    target: bionemo.model.protein.esm1nv.esm1nv_model.ESM1nvModel\n",
      "    infer_target: bionemo.model.protein.esm1nv.infer.ESM1nvInference\n",
      "    formatters:\n",
      "      simple:\n",
      "        format: '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'\n",
      "    handlers:\n",
      "      console:\n",
      "        class: logging.StreamHandler\n",
      "        formatter: simple\n",
      "        stream: ext://sys.stdout\n",
      "      file:\n",
      "        class: logging.FileHandler\n",
      "        formatter: simple\n",
      "        filename: /logs/inference.log\n",
      "    root:\n",
      "      level: INFO\n",
      "      handlers:\n",
      "      - console\n",
      "    disable_existing_loggers: false\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-11-07 00:02:45 save_restore_connector:394] src path does not exist or it is not a path in nemo file. src value I got was: /tokenizers/vocab/protein_sequence_sentencepiece.vocab. Absolute: /tokenizers/vocab/protein_sequence_sentencepiece.vocab\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: sequence_parallel in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-11-07 00:02:45 megatron_init:251] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2024-11-07 00:02:45 megatron_init:257] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2024-11-07 00:02:45 megatron_init:262] All data parallel group ranks with context parallel combined: [[0]]\n",
      "[NeMo I 2024-11-07 00:02:45 megatron_init:265] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2024-11-07 00:02:45 megatron_init:282] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2024-11-07 00:02:45 megatron_init:285] All context parallel group ranks: [[0]]\n",
      "[NeMo I 2024-11-07 00:02:45 megatron_init:286] Ranks 0 has context parallel rank: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1107 00:02:45.676885469 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-11-07 00:02:45 megatron_init:297] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2024-11-07 00:02:45 megatron_init:298] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-11-07 00:02:45 megatron_init:308] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2024-11-07 00:02:45 megatron_init:312] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-11-07 00:02:45 megatron_init:313] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2024-11-07 00:02:45 megatron_init:342] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2024-11-07 00:02:45 megatron_init:354] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2024-11-07 00:02:45 megatron_init:360] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-11-07 00:02:45 megatron_init:361] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2024-11-07 00:02:45 megatron_init:362] All embedding group ranks: [[0]]\n",
      "[NeMo I 2024-11-07 00:02:45 megatron_init:363] Rank 0 has embedding rank: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24-11-07 00:02:45 - PID:1181 - rank:(0, 0, 0, 0) - microbatches.py:39 - INFO - setting number of micro-batches to constant 1\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: sequence_parallel in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 save_restore_connector:394] src path does not exist or it is not a path in nemo file. src value I got was: /tokenizers/vocab/protein_sequence_sentencepiece.vocab. Absolute: /tokenizers/vocab/protein_sequence_sentencepiece.vocab\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-11-07 00:02:45 tokenizer_utils:191] Getting SentencePiece with model: /tmp/tmp2opp5leo/f9e5097b22ec4aac8849955c30fdb1c3_protein_sequence_sentencepiece.model\n",
      "[NeMo I 2024-11-07 00:02:45 megatron_base_model:544] Padded vocab_size: 128, original vocab_size: 30, dummy tokens: 98.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: sequence_parallel in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:1109] The model: ESM1nvModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:516] The model: ESM1nvModel() does not have field.name: num_query_groups in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:516] The model: ESM1nvModel() does not have field.name: attention_dropout in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:516] The model: ESM1nvModel() does not have field.name: add_qkv_bias in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:516] The model: ESM1nvModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:516] The model: ESM1nvModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:516] The model: ESM1nvModel() does not have field.name: persist_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:516] The model: ESM1nvModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:516] The model: ESM1nvModel() does not have field.name: fp8_margin in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:516] The model: ESM1nvModel() does not have field.name: fp8_interval in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:516] The model: ESM1nvModel() does not have field.name: fp8_amax_history_len in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:516] The model: ESM1nvModel() does not have field.name: fp8_amax_compute_algo in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:516] The model: ESM1nvModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:516] The model: ESM1nvModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:516] The model: ESM1nvModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:516] The model: ESM1nvModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:516] The model: ESM1nvModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:516] The model: ESM1nvModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:516] The model: ESM1nvModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:516] The model: ESM1nvModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-11-07 00:02:45 megatron_base_model:516] The model: ESM1nvModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-11-07 00:02:45 nlp_overrides:1140] Model ESM1nvModel was successfully restored from /workspace/bionemo/models/protein/esm1nv/esm1nv.nemo.\n",
      "Loaded a <class 'bionemo.model.protein.esm1nv.infer.ESM1nvInference'>\n"
     ]
    }
   ],
   "source": [
    "from bionemo.triton.utils import load_model_for_inference\n",
    "from bionemo.model.protein.esm1nv.infer import ESM1nvInference\n",
    "\n",
    "inferer = load_model_for_inference(cfg, interactive=True)\n",
    "\n",
    "print(f\"Loaded a {type(inferer)}\")\n",
    "assert isinstance(inferer, ESM1nvInference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a916ede0",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "find_con will find the first number in a string\n",
    "\n",
    "get_window will create a window for a certain idx of size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62aac3e9-b2e3-49d7-9cc3-8717028d4d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def find_con(s):\n",
    "    result = re.search('\\d+', s)\n",
    "    return result.group(0) if result else result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "219f4ee3-d46a-4ca9-8495-b759ee0af556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_window(arr, idx, size):\n",
    "    a = arr\n",
    "    idx = idx\n",
    "    N = size\n",
    "    if N % 2: # if window length is odd\n",
    "        step = N // 2\n",
    "    else: # if window length is even\n",
    "        step = int(N/2 - 1)\n",
    "    \n",
    "    # make sure starting index is between 0 and a.shape[0] - N\n",
    "    start = min(max(idx-step,0),len(a) - N)\n",
    "    window = a[start:start + N]\n",
    "    return window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9027370f",
   "metadata": {},
   "source": [
    "## Generate a new column called UUID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a99ff2d-134d-4ac9-970d-c8fa1530eef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate uuid\n",
    "df['uuid'] = df.apply(lambda _: uuid.uuid4(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6864f95f-201d-4310-a625-e2cd96c5083a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['uuid'] = df['uuid'].astype(\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20e2e69e-82c1-4a3c-81b0-905df5ae7550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['#CHROM', 'POS', 'VariationID', 'REF', 'ALT', 'AlleleID', 'GeneSymbol',\n",
       "       'GeneID', 'CLNHGVS', 'CLNSIG', 'CLNVC', 'Type', 'Name', 'RS# (dbSNP)',\n",
       "       'NucleotideExpression', 'NucleotideChange', 'ProteinChange', 'CDS',\n",
       "       'mCDS', 'Peptide', 'mPeptide', 'Dataset', 'uuid'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e12238",
   "metadata": {},
   "source": [
    "## Save the DataFrame as a .feather file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1e45399-1a22-4e1d-b157-4b709aa5c35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_feather(f\"{project_name}_uuid.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813559eb",
   "metadata": {},
   "source": [
    "## Finally, load the file and perform inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d126b3e6-9f88-4301-bcb1-1d7caf573bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_feather(f\"{project_name}_uuid.feather\")\n",
    "os.makedirs(f\"./{project_name}\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "787441fc-37f7-4c81-9cbc-b5af375e3c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    try:\n",
    "        win = int(find_con(row[\"ProteinChange\"]))\n",
    "    except Exception:\n",
    "        win = 0\n",
    "        print(row[\"ProteinChange\"], row['uuid'])\n",
    "    a = get_window(row[\"mPeptide\"], win, 512)[:-2]\n",
    "    if not Path(f\"./{project_name}/{row['uuid']}.npy\").is_file():\n",
    "        try:\n",
    "            res = inferer.seq_to_embeddings([a]) # Inference\n",
    "        except Exception:\n",
    "            print(\"API Error:\", row[\"ProteinChange\"], row['uuid'])\n",
    "            continue\n",
    "        np.save(f\"./{project_name}/{row['uuid']}\", res[0].cpu().numpy()) # Save as NPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929f0fde-b271-469f-bf24-12a9cb18b121",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c659d534-d9cb-442e-a1d8-62e73493da04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
